{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cleanse data and construct the model base on the results of EDA.\n",
    "\n",
    "First, I will create a baseline model. It puts data right into the model, which is LinearRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_raw = pd.read_csv('train.csv')\n",
    "test_raw = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_data(train_set, test_set, test_has_label=True):\n",
    "\n",
    "    \n",
    "\n",
    "    # 1. delete 'brand'\n",
    "    train_set = train_set.drop(['brand'], axis=1)\n",
    "    test_set = test_set.drop(['brand'], axis=1)\n",
    "\n",
    "    # 3. drop ext_col, int_col\n",
    "    train_set = train_set.drop(['ext_col', 'int_col'], axis=1)\n",
    "    test_set = test_set.drop(['ext_col', 'int_col'], axis=1)\n",
    "\n",
    "    # 4. drop id\n",
    "    train_set = train_set.drop(['id'], axis=1)\n",
    "    test_set = test_set.drop(['id'], axis=1)\n",
    "\n",
    "    ''' only 8 features ('model_year', 'milage', 'fuel_type', 'accident', 'clean_title', 'model', 'engine') left '''\n",
    "\n",
    "    # 5. for the same model value group, delete all rows that has high price (above 90% quantile).\n",
    "    quantiles = train_set.groupby('model')['price'].quantile(0.9)\n",
    "    train_set = train_set[train_set.apply(lambda row: row['price'] <= quantiles[row['model']], axis=1)]\n",
    "    \n",
    "    # 6. Filter out rows that has price higher than 500K\n",
    "    train_set = train_set[train_set.apply(lambda row: row['price'] < 500000, axis=1)]\n",
    "\n",
    "    # 7. convert 'year' to int, and calculate year passed from min(year)\n",
    "    base = min(train_set['model_year'])\n",
    "    train_set['model_year'] = (train_set['model_year'].astype(int) - base)\n",
    "    test_set['model_year']  = (test_set['model_year'] .astype(int) - base)\n",
    "    \n",
    "    # 9. Aggregate categorical features in transmission\n",
    "    col_names = [\n",
    "    'A/T',\n",
    "    'Transmission w/Dual Shift Mode',\n",
    "    '7-Speed A/T',\n",
    "    '8-Speed A/T',\n",
    "    '10-Speed Automatic',\n",
    "    '1-Speed A/T',\n",
    "    '6-Speed A/T',\n",
    "    '10-Speed A/T',\n",
    "    '9-Speed A/T',\n",
    "    '8-Speed Automatic',\n",
    "    '9-Speed Automatic',\n",
    "    '5-Speed A/T',\n",
    "    'Automatic',\n",
    "    '7-Speed Automatic with Auto-Shift',\n",
    "    'CVT Transmission',\n",
    "    '5-Speed M/T',\n",
    "    'M/T',\n",
    "    '6-Speed M/T',\n",
    "    '6-Speed Automatic',\n",
    "    '4-Speed Automatic',\n",
    "    '7-Speed M/T',\n",
    "    '2-Speed A/T',\n",
    "    '1-Speed Automatic',\n",
    "    'Automatic CVT',\n",
    "    '4-Speed A/T',\n",
    "    '6-Speed Manual',\n",
    "    'Transmission Overdrive Switch',\n",
    "    '8-Speed Automatic with Auto-Shift',\n",
    "    '7-Speed Manual',\n",
    "    '7-Speed Automatic',\n",
    "    '9-Speed Automatic with Auto-Shift',\n",
    "    '6-Speed Automatic with Auto-Shift',\n",
    "    '6-Speed Electronically Controlled Automatic with O',\n",
    "    'F',\n",
    "    'CVT-F',\n",
    "    '8-Speed Manual',\n",
    "    'Manual',\n",
    "    '-',\n",
    "    '2',\n",
    "    '6 Speed At/Mt',\n",
    "    '5-Speed Automatic',\n",
    "    '2-Speed Automatic',\n",
    "    '8-SPEED A/T',\n",
    "    '7-Speed',\n",
    "    'Variable',\n",
    "    'Single-Speed Fixed Gear',\n",
    "    '8-SPEED AT',\n",
    "    '10-Speed Automatic with Overdrive',\n",
    "    '7-Speed DCT Automatic',\n",
    "    'SCHEDULED FOR OR IN PRODUCTION',\n",
    "    '6-Speed',\n",
    "    '6 Speed Mt'\n",
    "]\n",
    "    \n",
    "    col_names_override = [\n",
    "    'A/T',\n",
    "    'Transmission w/Dual Shift Mode',\n",
    "    '7-Speed A/T',\n",
    "    '8-Speed A/T',\n",
    "    '10-Speed A/T',\n",
    "    '1-Speed A/T',\n",
    "    '6-Speed A/T',\n",
    "    '10-Speed A/T',\n",
    "    '9-Speed A/T',\n",
    "    '8-Speed A/T',\n",
    "    '9-Speed A/T',\n",
    "    '5-Speed A/T',\n",
    "    'A/T',\n",
    "    '7-Speed A/T with Auto-Shift',\n",
    "    'CVT Transmission',\n",
    "    '5-Speed M/T',\n",
    "    'M/T',\n",
    "    '6-Speed M/T',\n",
    "    '6-Speed A/T',\n",
    "    '4-Speed A/T',\n",
    "    '7-Speed M/T',\n",
    "    '2-Speed A/T',\n",
    "    '1-Speed A/T',\n",
    "    'A/T CVT',\n",
    "    '4-Speed A/T',\n",
    "    '6-SpeedM/T',\n",
    "    'Transmission Overdrive Switch',\n",
    "    '8-Speed A/T with Auto-Shift',\n",
    "    '7-Speed M/T',\n",
    "    '7-Speed A/T',\n",
    "    '9-Speed A/T with Auto-Shift',\n",
    "    '6-Speed A/T with Auto-Shift',\n",
    "    '6-Speed Electronically Controlled A/T with O',\n",
    "    '-',\n",
    "    'CVT-F',\n",
    "    '8-Speed M/T',\n",
    "    'M/T',\n",
    "    '-',\n",
    "    '-',\n",
    "    '6 Speed At/Mt',\n",
    "    '5-Speed A/T',\n",
    "    '2-Speed A/T',\n",
    "    '8-Speed A/T',\n",
    "    '7-Speed',\n",
    "    'Variable',\n",
    "    'Single-Speed Fixed Gear',\n",
    "    '8-Speed AT',\n",
    "    '10-Speed A/T with Overdrive',\n",
    "    '7-Speed DCT A/T',\n",
    "    '-',\n",
    "    '6-Speed',\n",
    "    '6 Speed M/T'\n",
    "]\n",
    "    \n",
    "    trans_dict = dict(zip(col_names, col_names_override))\n",
    "    train_set['transmission'] = train_set['transmission'].replace(trans_dict)\n",
    "    test_set['transmission']  = test_set['transmission'].replace(trans_dict)\n",
    "\n",
    "    # Get all numeric values\n",
    "    # I will add categoricals later\n",
    "    \n",
    "\n",
    "    \n",
    "    categoricals = ['accident', 'clean_title']#['transmission', 'fuel_type', 'accident', 'clean_title', 'model', 'engine']\n",
    "\n",
    "\n",
    "    data_average = train_set.groupby('model')['price'].mean().reset_index()\n",
    "    # print(data_average.shape)\n",
    "    data_average.columns = ['model', 'average_price']\n",
    "    print(data_average.head())\n",
    "    print(train_set.head())\n",
    "    replace_dict = dict(zip(data_average['model'], data_average['average_price']))\n",
    "    train_set['model'] = train_set['model'].replace(replace_dict)\n",
    "    print(train_set.head())\n",
    "    \n",
    "    # for the test data too:\n",
    "    data_average = train_set.groupby('model')['price'].mean().reset_index()\n",
    "    # print(data_average.shape)\n",
    "    data_average.columns = ['model', 'average_price']\n",
    "    test_set['model'] = test_set['model'].replace(replace_dict)\n",
    "    test_set['model'] = test_set['model'].apply(lambda x: 20000 if isinstance(x, str) else x)\n",
    "\n",
    "    train_processed = train_set.loc[:, ('model', 'milage', 'model_year', 'price')]\n",
    "    if test_has_label:\n",
    "        test_processed = test_set.loc[:, ('model', 'milage', 'model_year', 'price')]\n",
    "    else:\n",
    "        test_processed = test_set.loc[:, ('model', 'milage', 'model_year')]\n",
    "\n",
    "    # print(train_set.columns)\n",
    "\n",
    "    for i in categoricals:\n",
    "\n",
    "        train_set_part = train_set[i]\n",
    "        test_set_part = test_set[i]\n",
    "\n",
    "        train_set_part = train_set_part.fillna('blank')\n",
    "        test_set_part  = test_set_part.fillna('blank')\n",
    "\n",
    "        # set values of train_set[i] to 'blank', 1% of them\n",
    "        train_set_part.loc[train_set.sample(frac=0.01).index] = 'blank'\n",
    "\n",
    "        # Get all unique values in train_set\n",
    "        train_value_set = set(train_set_part)\n",
    "\n",
    "        # get_dummies for train's model\n",
    "        train_set_encoded = pd.get_dummies(train_set_part, prefix=i)\n",
    "\n",
    "        # Remove categories that are not in train, switch them to 'blank'\n",
    "        test_set[i] = test_set[i].apply(lambda x: 'blank' if x not in train_value_set else x)\n",
    "        test_set_encoded = pd.get_dummies(test_set[i], prefix=i)\n",
    "\n",
    "        # Add columns for train set\n",
    "        for j in train_set_encoded.columns:\n",
    "            if j not in test_set_encoded.columns:\n",
    "                test_set_encoded[j] = 0\n",
    "        \n",
    "        # Add blank columns if it was not created\n",
    "        if i+'_blank' not in train_set_encoded.columns:\n",
    "            train_set_encoded[i+'_blank'] = 0\n",
    "        if i+'_blank' not in test_set_encoded.columns:\n",
    "            test_set_encoded[i+'_blank'] = 0\n",
    "        \n",
    "        train_processed = pd.concat((train_processed, train_set_encoded), axis=1)\n",
    "        test_processed  = pd.concat((test_processed,  test_set_encoded), axis=1)\n",
    "\n",
    "    # sort columns\n",
    "    train_processed = train_processed[train_processed.columns.sort_values()]\n",
    "    test_processed  = test_processed[test_processed.columns.sort_values()]\n",
    "\n",
    "    print(train_processed.head())\n",
    "    \n",
    "\n",
    "    return train_processed, test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Refactoring of the function above...\n",
    "'''\n",
    "import copy\n",
    "\n",
    "def cleanse_data(train_set, test_set, test_has_label=True):\n",
    "\n",
    "    train_set = copy.deepcopy(train_set)\n",
    "    test_set = copy.deepcopy(test_set)\n",
    "    \n",
    "    # 1. delete 'brand'. This data is incorrect so often.\n",
    "    train_set = train_set.drop(['brand'], axis=1)\n",
    "    test_set = test_set.drop(['brand'], axis=1)\n",
    "\n",
    "    # 3. drop ext_col, int_col\n",
    "    train_set = train_set.drop(['ext_col', 'int_col'], axis=1)\n",
    "    test_set = test_set.drop(['ext_col', 'int_col'], axis=1)\n",
    "\n",
    "    # 4. drop id\n",
    "    train_set = train_set.drop(['id'], axis=1)\n",
    "    test_set = test_set.drop(['id'], axis=1)\n",
    "\n",
    "    ''' only 8 features ('model_year', 'milage', 'fuel_type', 'accident', 'clean_title', 'model', 'engine') left '''\n",
    "\n",
    "    # 5. for the same model value group, delete all rows that has high price (above 90% quantile).\n",
    "    quantiles = train_set.groupby('model')['price'].quantile(0.9)\n",
    "    train_set = train_set[train_set.apply(lambda row: row['price'] <= quantiles[row['model']], axis=1)]\n",
    "    \n",
    "    # 6. Filter out rows that has price higher than 500K\n",
    "    train_set = train_set[train_set.apply(lambda row: row['price'] < 500000, axis=1)]\n",
    "\n",
    "    # 7. convert 'year' to int, and calculate year passed from min(year)\n",
    "    base = min(train_set['model_year'])\n",
    "    train_set['model_year'] = (train_set['model_year'].astype(int) - base)**2\n",
    "    test_set['model_year']  = (test_set['model_year'] .astype(int) - base)**2\n",
    "\n",
    "    # convert model name to the average of price\n",
    "    data_average = train_set.groupby('model')['price'].mean().reset_index()\n",
    "    data_average.columns = ['model', 'average_price']\n",
    "    replace_dict = dict(zip(data_average['model'], data_average['average_price']))\n",
    "\n",
    "    train_set['model'] = train_set['model'].replace(replace_dict)\n",
    "    test_set['model'] = test_set['model'].replace(replace_dict)\n",
    "\n",
    "    # If there is a model that was not in train set, set it to 20000\n",
    "    test_set['model'] = test_set['model'].apply(lambda x: 20000 if isinstance(x, str) else x)\n",
    "\n",
    "    # Change accident value\n",
    "    train_set.loc[train_set.sample(frac=0.01).index, 'accident'] = 'blank'\n",
    "    replace_dict = dict(zip(['At least 1 accident or damage reported', 'blank', 'None reported'], [1, 1, 0]))\n",
    "    train_set['accident'] = train_set['accident'].replace(replace_dict)\n",
    "    train_set['accident'].fillna(1, inplace=True)\n",
    "    test_set['accident'] = test_set['accident'].replace(replace_dict)\n",
    "    train_set['accident'].fillna(1, inplace=True)\n",
    "\n",
    "    # Change clean_title value\n",
    "    train_set.loc[train_set.sample(frac=0.01).index, 'clean_title'] = 'blank'\n",
    "    replace_dict = dict(zip(['Yes', 'blank', None], [1, 0, 0]))\n",
    "    train_set['clean_title'] = train_set['clean_title'].replace(replace_dict)\n",
    "    train_set['clean_title'].fillna(0, inplace=True)\n",
    "    test_set['clean_title'] = test_set['clean_title'].replace(replace_dict)\n",
    "    test_set['clean_title'].fillna(0, inplace=True)\n",
    "\n",
    "    train_set.drop(['engine', 'transmission', 'fuel_type'], axis=1, inplace=True)\n",
    "    test_set.drop(['engine', 'transmission', 'fuel_type'], axis=1, inplace=True)\n",
    "\n",
    "    for i in ['model', 'milage', 'model_year']:\n",
    "        scaler = StandardScaler()\n",
    "        train_set[i] = scaler.fit_transform(train_set[i].values.reshape(-1, 1))\n",
    "        test_set[i]  = scaler.transform(test_set[i].values.reshape(-1, 1))\n",
    "\n",
    "    return train_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train, test):\n",
    "\n",
    "    train = copy.deepcopy(train)\n",
    "    test = copy.deepcopy(test)\n",
    "\n",
    "    print(train.columns)\n",
    "\n",
    "    train = train.drop(columns='ext_col', axis=1)\n",
    "    train = train.drop(columns='int_col', axis=1)\n",
    "    train = train.drop(columns='brand', axis=1)\n",
    "\n",
    "    quantiles = train.groupby('model')['price'].quantile(0.9)\n",
    "    train = train[train.apply(lambda row: row['price'] <= quantiles[row['model']], axis=1)]\n",
    "    \n",
    "    # 6. Filter out rows that has price higher than 500K\n",
    "    train = train[train.apply(lambda row: row['price'] < 500000, axis=1)]\n",
    "    \n",
    "    TRAIN_LN = int(len(train)*0.8)\n",
    "    CATS = [c for c in train.columns if not c in [\"id\",\"price\"] ]\n",
    "    NUMS = ['milage']\n",
    "    CATS = [c for c in CATS if not c in NUMS]\n",
    "    print(\"Categorical features:\", CATS )\n",
    "    print(\"Numerical features:\", NUMS)\n",
    "    print(\"STANDARDIZING: \",end=\"\")\n",
    "    for c in NUMS:\n",
    "        print(c,', ',end='')\n",
    "        m = train[c].mean()\n",
    "        s = train[c].std()\n",
    "        train[c] = (train[c]-m)/s\n",
    "        train[c] = train[c].fillna(m)\n",
    "    CAT_SIZE = []\n",
    "    CAT_EMB = []\n",
    "    RARE = []\n",
    "\n",
    "    print(\"LABEL ENCODING:\")\n",
    "    for c in CATS:\n",
    "        # LABEL ENCODE\n",
    "        train[c],_ = train[c].factorize()\n",
    "        train[c] -= train[c].min()\n",
    "        vc = train[c].value_counts()\n",
    "        \n",
    "        # IDENTIFY RARE VALUES\n",
    "        RARE.append( vc.loc[vc<40].index.values )\n",
    "        n = train[c].nunique()\n",
    "        mn = train[c].min()\n",
    "        mx = train[c].max()\n",
    "        r = len(RARE[-1])\n",
    "        print(f'{c}: nunique={n}, min={mn}, max={mx}, rare_ct={r}')\n",
    "        \n",
    "        # RELABEL RARE VALUES AS ZERO\n",
    "        CAT_SIZE.append(mx+1 +1) #ADD ONE FOR RARE\n",
    "        CAT_EMB.append( int(np.ceil( np.sqrt(mx+1 +1))) ) # ADD ONE FOR RARE\n",
    "        train[c] += 1\n",
    "        train.loc[train[c].isin(RARE[-1]),c] = 0 \n",
    "\n",
    "    # # test = train.iloc[TRAIN_LN:]\n",
    "    # train = train.iloc[:TRAIN_LN]\n",
    "    \n",
    "    for c in CATS:\n",
    "        # COMPARE TEST CAT VALUES TO TRAIN CAT VALUES\n",
    "        A = train[c].unique()\n",
    "        B = test[c].unique()\n",
    "        print(A, B)\n",
    "        C = np.setdiff1d(B,A)\n",
    "        print(f\"{c}: Test has label encodes = {C} which are not in train.\")\n",
    "        if len(C)>0:\n",
    "            print(f\" => {len(test.loc[test[c].isin(C)])} rows\" )\n",
    "            \n",
    "        # RELABEL UNSEEN TEST VALUES AS ZERO\n",
    "        test.loc[test[c].isin(C),c] = 0 \n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'brand', 'model', 'model_year', 'milage', 'fuel_type', 'engine',\n",
      "       'transmission', 'ext_col', 'int_col', 'accident', 'clean_title',\n",
      "       'price'],\n",
      "      dtype='object')\n",
      "Categorical features: ['model', 'model_year', 'fuel_type', 'engine', 'transmission', 'accident', 'clean_title']\n",
      "Numerical features: ['milage']\n",
      "STANDARDIZING: milage , LABEL ENCODING:\n",
      "model: nunique=1897, min=0, max=1896, rare_ct=979\n",
      "model_year: nunique=34, min=0, max=33, rare_ct=2\n",
      "fuel_type: nunique=8, min=0, max=7, rare_ct=1\n",
      "engine: nunique=1116, min=0, max=1115, rare_ct=501\n",
      "transmission: nunique=52, min=0, max=51, rare_ct=17\n",
      "accident: nunique=3, min=0, max=2, rare_ct=0\n",
      "clean_title: nunique=2, min=0, max=1, rare_ct=0\n",
      "[   0    2    3    4    5    7    8    9   10   11   12   13   14   15\n",
      "   16   17   18   19   20   21   22   24   25   26   27   28   29   30\n",
      "   31   32   33   34   35   36   37   38   39   40   41   42   43   44\n",
      "   45   46   47   48   49   50   52   53   54   55   56   57   58   59\n",
      "   60   61   62   63   64   65   66   67   68   69   70   71   73   74\n",
      "   75   76   77   78   79   80   82   83   84   85   86   87   88   89\n",
      "   90   91   92   94   95   96   97   99  100  101  102  105  106  107\n",
      "  108  109  110  112  113  114  115  116  117  118  119  120  121  122\n",
      "  123  124  125  126  127  128  129  130  131  132  133  134  135  136\n",
      "  138  139  140  143  144  145  146  147  148  149  151  152  154  155\n",
      "  156  157  158  159  160  162  163  164  165  166  168  169  170  171\n",
      "  173  174  175  176  177  178  179  180  181  182  183  184  185  186\n",
      "  187  190  192  193  194  195  196  197  198  199  200  201  202  203\n",
      "  204  205  206  207  209  210  211  212  213  214  217  218  223  224\n",
      "  225  227  228  229  230  232  233  234  235  236  237  238  240  241\n",
      "  242  243  244  245  246  247  248  250  252  254  255  256  258  259\n",
      "  260  261  263  265  266  267  268  269  271  272  273  274  275  276\n",
      "  277  278  279  281  282  283  284  285  286  288  290  291  292  293\n",
      "  294  295  296  299  300  301  302  303  305  306  307  308  309  310\n",
      "  311  312  313  315  316  317  318  319  320  322  325  327  328  329\n",
      "  332  333  335  336  337  338  339  340  341  342  345  347  350  351\n",
      "  352  353  354  355  356  359  360  361  362  363  364  365  366  367\n",
      "  368  369  370  371  372  373  374  375  376  377  378  379  380  381\n",
      "  383  385  386  387  388  391  392  394  395  396  397  398  399  400\n",
      "  401  402  404  405  406  407  408  411  413  414  416  418  419  420\n",
      "  421  422  423  425  426  427  428  429  430  431  432  433  434  435\n",
      "  436  437  438  439  440  442  443  445  447  448  450  451  453  454\n",
      "  455  457  459  460  462  463  465  467  468  469  470  473  474  475\n",
      "  478  479  480  481  482  484  486  487  488  489  490  491  493  494\n",
      "  495  496  497  498  500  501  502  504  505  506  508  510  511  512\n",
      "  513  516  518  520  521  522  523  524  528  529  530  531  532  534\n",
      "  536  538  539  540  541  542  543  544  547  548  549  550  551  553\n",
      "  554  555  556  557  558  560  562  563  564  565  566  567  568  569\n",
      "  570  571  572  573  574  575  576  579  580  582  585  586  587  590\n",
      "  591  595  596  597  599  600  601  602  603  606  608  609  611  614\n",
      "  615  616  618  619  621  622  623  624  627  628  629  635  636  637\n",
      "  639  640  642  643  644  645  648  649  650  651  654  656  659  660\n",
      "  661  662  663  664  665  666  667  668  669  670  672  673  674  676\n",
      "  677  679  680  681  682  683  684  685  686  690  691  692  693  694\n",
      "  696  698  699  700  701  702  703  705  706  708  711  712  713  715\n",
      "  716  717  720  723  724  727  729  733  735  737  740  742  743  744\n",
      "  745  746  751  753  754  755  756  758  760  761  764  765  767  768\n",
      "  769  771  772  773  775  776  778  779  785  786  787  788  789  790\n",
      "  791  792  795  796  798  799  800  801  803  804  805  807  808  810\n",
      "  811  813  816  817  818  819  820  821  822  823  824  826  828  830\n",
      "  832  833  834  836  837  839  842  844  845  847  849  850  851  852\n",
      "  853  855  856  857  858  860  862  864  865  866  869  870  872  873\n",
      "  874  875  876  879  881  882  883  886  888  889  890  891  893  895\n",
      "  896  898  900  901  902  906  907  908  913  914  916  917  919  920\n",
      "  922  925  927  931  932  933  934  935  936  938  939  941  945  947\n",
      "  950  951  952  957  958  959  960  961  962  963  964  966  968  971\n",
      "  972  973  975  977  978  979  980  983  985  988  989  995  998  999\n",
      " 1002 1003 1004 1006 1008 1009 1011 1017 1018 1022 1023 1026 1030 1031\n",
      " 1036 1038 1039 1040 1043 1044 1045 1047 1054 1056 1060 1063 1064 1065\n",
      " 1067 1069 1073 1077 1078 1080 1082 1084 1087 1088 1090 1094 1095 1098\n",
      " 1100 1102 1106 1107 1109 1110 1113 1114 1117 1122 1124 1126 1127 1129\n",
      " 1130 1132 1136 1139 1140 1143 1149 1150 1153 1154 1155 1156 1157 1161\n",
      " 1167 1170 1171 1176 1178 1180 1186 1189 1190 1193 1197 1198 1201 1203\n",
      " 1207 1212 1215 1219 1220 1222 1223 1225 1229 1230 1233 1234 1236 1237\n",
      " 1238 1239 1246 1247 1249 1250 1251 1254 1257 1258 1259 1260 1261 1262\n",
      " 1264 1265 1266 1267 1269 1271 1275 1278 1281 1282 1283 1287 1290 1291\n",
      " 1298 1300 1301 1302 1306 1312 1316 1322 1326 1330 1333 1337 1338 1339\n",
      " 1340 1344 1351 1354 1357 1370 1371 1375 1377 1378 1386 1390 1396 1403\n",
      " 1407 1411 1419 1422 1423 1427 1428 1435 1439 1448 1449 1451 1456 1459\n",
      " 1475 1497 1506 1508 1509 1525 1532 1533 1536 1538 1545 1550 1563 1587\n",
      " 1606 1615 1625 1634 1674 1691 1711 1724] [  36    0  164   62  640    9  243   90  340  123  569   32  772  114\n",
      "  269  147   50  901  662  293  319  858  669   88   77  356  482  185\n",
      "  105  338   39  623  271   31  202  236  157   37  183  276   10  555\n",
      "  365  163  102  572  160 1250 1247   67  830  140  110  238   53  541\n",
      "   45  755   61  107  591   40  246   29  834  213  373   86  193  307\n",
      "  336  817   26  796  408  619  875  283   28   14  175  187  313   75\n",
      " 1301  159  494  511    5 1023  278  959  133 1449  580 1291  145  122\n",
      "   76  364  962 1002  131  233   54  932  115  490   30  407  218  889\n",
      "  516  419   44  479   79  291  543  311   35  155  210  360   87   97\n",
      "   58  205  248  693 1094   11  818   43   13   19  789  700   16  666\n",
      "  363  316  786  430   48  176   84  865  414  101  463  369  973  352\n",
      "  447   65  128  124 1525  448  845 1351  305  100   95  292  245  399\n",
      "  874  301  120  203  328  475  540  775  277   60   38  616  522   15\n",
      "  746 1026  686  126  166   71  169  318  856  436  118  800  406  860\n",
      "  668  286 1207  914  366 1065  184  194   78  565  756  890  317 1171\n",
      "  523 1378  799  282  119 1110  379  156  438  665  788  152   49  443\n",
      "   91  664   22  198  539  701  117   68 1018  643 1330  392  624  339\n",
      "  229  426  512  400  146  362  350 1333   64  510  135  947  459  963\n",
      " 1132   21   47  733 1107  237  320 1220 1233  851  601  165 1338  713\n",
      "  644  650  554  433  828  857  852 1117  420  497  425  148 1044   55\n",
      " 1136  699  335  109  310 1258  506 1189   20  706  259  143  532  821\n",
      "  645  377  596 1290  754  274  978  684   57  667  661  303  549  869\n",
      "  866  199  787  130  132  470   42 1193  819  481 1203  570 1251  285\n",
      "  139  536  465  134  769  600 1533  609  308  563   25  186  595   46\n",
      "  439  585   18  960   89  309  149  579  773 1281 1036  398  211 1222\n",
      " 1060  677  702  568  628  823  833  273  341  462  108  206  931   63\n",
      " 1223  312  351  558  179  247  712  920 1190  602  428  268  423 1357\n",
      "  460  432  306 1691  121  939  244  615  260  209 1155  573  214  361\n",
      "  501 1239  241  242 1456  300  112   17 1009   80 1225  743  422  649\n",
      "  223  761  574  255 1078   92  576 1056 1448  681 1127  401  294  353\n",
      "  180  663  394  144  870  977   27  792 1606 1275  480  698 1238  496\n",
      "  232  230  849  557  201  801  627  489  254  129  387  724  178 1422\n",
      "  371 1262  421  127  836  267  705  469  279  125  170  564  182  380\n",
      "   33  717  431  691   66  542  197  342  917 1106 1095 1260  622  988\n",
      "  694  195  635  505  427 1154  385  964   12  566  354  234  378  636\n",
      "  683  429  225  375   69 1545  113 1371  488 1039  440   56   59   41\n",
      "  256  174 1497 1230  500  637  367 1339  820  402  106  844  544  676\n",
      "  265  227  295  455 1451  873  968  945  895  531  590    2  837  933\n",
      "  586 1087  779  181  359  329  513 1354  434  811  299 1114  902  587\n",
      "  614  853   74  673  881  776 1427 1064 1084  381  207  979  474  813\n",
      "  983 1156 1261  495   85  961  832  679  692  941   83  374  168  498\n",
      "  508  116  765 1043  370  660  680 1407  524  302  263  177  451  200\n",
      "  217 1011   96 1282  395  445  999    3  173 1265  778   70  872  575\n",
      "  261  927  958  980 1563 1003 1435  791  355  550  502 1031 1229  952\n",
      " 1283  372  629  822  597  883 1269  896  386  674  716 1340  876 1271\n",
      " 1587  760  405 1302  907  768  530  672  850  435 1428  571  651  204\n",
      "  925  935 1711 1098  413  151  764  212  158  288 1180  250 1090   52\n",
      "    4  690  562 1126  816  347  272 1088  136  504 1047  685  682  751\n",
      " 1219 1063 1249  618  972  742 1237  670 1054  798  493   34 1287  908\n",
      " 1102  608 1008 1439 1509  454  803 1176 1423 1170  521 1069  376  266\n",
      "   73  934 1186  457 1266  487  842  486  154 1897 1073 1157  735  468\n",
      " 1322 1030 1234  333  985 1004  437  296  998 1259  795  708  785  453\n",
      "  258  416  397  696  418  190  886  224  891  228  989  826  745  723\n",
      "  235  839 1615  639 1298  966 1167  882  656  938  824  275  171  900\n",
      "  322 1375 1532  388  196 1161  478  727 1403  582  648  642 1045  864\n",
      "  551 1149  936  808 1411  327 1267  922  290  916 1178 1140  345  240\n",
      "  383  603  560  396   82 1124  491  138  807  518  484  284  528  790\n",
      "  805 1212    8  553  315  467 1634  758 1386  534    7 1201 1150 1246\n",
      " 1130  975  703 1377 1236  548 1475  898 1067  567  737 1040  337  529\n",
      " 1625 1122  332 1038  599  538 1198  951   94 1536  919  621   99  556\n",
      " 1006 1508  473  950  771 1674 1113 1300  767  855 1129  368 1215  906\n",
      " 1254  888  659  913 1344 1080 1077 1153  192  711  879 1022 1100  804\n",
      " 1506  810  744 1109 1370  654  411  893  162  740 1312  720  520 1278\n",
      " 1082 1390 1316  729  442 1257 1306 1419  252  753  325 1459 1724  862\n",
      "  715 1396  957  404 1017  547  450   24  281 1264 1337  847]\n",
      "model: Test has label encodes = [1897] which are not in train.\n",
      " => 1 rows\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31  0 34] [15  8 20  1  4 21 19 13  9 17 14 12  6 11  7  3 23 22 24 18  5 25 26 30\n",
      " 10  2 16 29 27  0 28 31 34]\n",
      "model_year: Test has label encodes = [] which are not in train.\n",
      "[2 3 1 4 5 6 0 8] [2 6 4 5 1 3 0 8]\n",
      "fuel_type: Test has label encodes = [] which are not in train.\n",
      "[   1    2    3    4    5    6    7    8    9   10   11   12   13   14\n",
      "   15   16   17   18   19   20   21   22   23   24   25   26   27   28\n",
      "   29   30   31   32   33   34   35   36   37   38   39   40   41   42\n",
      "   43   44   45   46   47   48   49    0   52   53   54   55   56   57\n",
      "   58   59   60   61   62   63   64   65   67   68   69   70   71   72\n",
      "   73   74   75   76   77   78   79   80   81   82   83   84   85   86\n",
      "   87   88   89   90   91   95   96   98   99  100  101  102  103  104\n",
      "  105  106  107  108  109  111  112  113  114  115  116  117  118  119\n",
      "  120  121  122  123  124  125  126  128  129  130  131  132  133  134\n",
      "  135  137  138  139  140  141  143  144  145  146  147  148  149  150\n",
      "  151  152  153  154  155  156  157  158  159  160  161  163  164  165\n",
      "  166  168  169  170  171  172  173  174  175  176  177  178  179  180\n",
      "  181  182  183  184  186  187  189  190  191  192  193  194  195  196\n",
      "  197  199  200  201  202  203  204  205  206  207  208  209  210  211\n",
      "  212  213  214  215  216  217  218  219  220  221  222  223  224  226\n",
      "  228  229  230  231  232  233  234  235  237  238  239  240  241  243\n",
      "  245  246  247  248  249  250  251  252  253  254  255  256  257  258\n",
      "  260  262  263  264  267  268  269  270  271  272  273  274  275  276\n",
      "  277  278  279  280  281  282  283  284  286  287  288  289  290  291\n",
      "  292  293  294  295  296  297  298  299  300  301  302  303  304  305\n",
      "  306  308  310  312  313  315  316  317  318  319  323  324  325  326\n",
      "  327  328  329  330  331  332  333  334  335  336  338  341  342  343\n",
      "  344  345  347  348  349  350  351  352  353  354  355  356  357  359\n",
      "  360  361  362  363  365  366  367  368  370  371  372  373  374  375\n",
      "  376  379  381  382  383  384  385  386  387  388  389  390  391  392\n",
      "  393  394  396  397  398  399  400  401  403  404  405  408  410  411\n",
      "  412  413  414  416  417  418  420  422  423  425  426  428  429  430\n",
      "  431  432  433  434  435  436  438  440  442  444  445  446  447  448\n",
      "  449  450  451  452  454  455  456  457  458  459  460  461  463  465\n",
      "  467  468  469  470  472  474  479  481  482  484  485  486  488  490\n",
      "  491  492  493  495  496  497  499  500  501  502  503  508  509  510\n",
      "  511  515  518  519  520  521  523  524  525  526  528  529  530  531\n",
      "  532  533  534  535  536  538  540  542  544  545  547  548  549  550\n",
      "  551  552  553  554  555  557  562  563  564  565  566  567  568  571\n",
      "  572  575  576  577  578  580  581  583  584  585  588  590  591  593\n",
      "  597  598  599  600  601  602  603  604  605  606  609  610  611  612\n",
      "  614  615  616  617  618  621  622  623  625  626  627  628  629  630\n",
      "  634  635  637  638  639  641  642  644  646  648  649  650  652  654\n",
      "  655  656  657  658  659  660  663  664  665  667  670  671  673  676\n",
      "  678  679  683  685  687  690  691  692  697  700  701  704  705  711\n",
      "  713  714  717  719  720  724  728  732  740  743  750  753  757  767\n",
      "  771  772  773  774  776  777  778  781  784  786  787  788  790  794\n",
      "  797  798  804  807  809  814  816  818  819  821  825  828  839  842\n",
      "  862  866  880  884  885  895  903  931  941  947  958  978 1044] [  33    0   29  133   58  568   13   62  193    5    7  144   35   31\n",
      "  124  599   16  217   45   40  316   21  461  232   57  149  334   75\n",
      "   48   38  931  161   41   69  222  150  362  192    6  219   18   42\n",
      "   32   10   46   44  220   86  387  206  288   55  145  130   43  449\n",
      "  294  140  534  120  555   88   12  532  197  182  635  171  571   74\n",
      "  240  141  233   24  212    3  177  218  238   26   99  176  152  656\n",
      "  157   73  679  155  290  129  196  254  190  131  108   61  271  670\n",
      "  117  338  628  445  354  318  777  276  100  414   28   80  384  146\n",
      "  204  772  224  591  234  501   34  216  552  545  126  210  175   54\n",
      "   27  200   96  509  618   17   19  181   15  382  659  281   56   30\n",
      "  169  399  313  229  119  207   68  106   85   37  617  392  203  195\n",
      "  397   91   60  231  278  184  103  413  230    1  208   78  562   53\n",
      "  304   14  143  429  165  298   81  246  138  550  325  576  778  102\n",
      "  257   72  151   52  251  356  642  390  371   47  104  213  348  194\n",
      "   25   20  299   65  493  201  160  112  434  593  153  404  139  536\n",
      "  577  118  457  482   77  156  500  134   84  164  547  616  237  649\n",
      "   11  630  533  391  274  191  678  317  135   82  612  456  272  269\n",
      "  862  241  623  394  601  282  252  292  115   70  327  787  588  525\n",
      "  132   83  214  349  383  287  430  351  284  297  448  423  174   22\n",
      "  418  639  335  452  109  629  529  657  148   36  223   79  701  385\n",
      "  663  446   23  280  113  788  374  447  363   76  122  885  128  166\n",
      "  256  435  170  258  398  291  270  496  605  350  319  603  658  819\n",
      "  660  563  692  685  352  324  277  523  301  524    8  268  303  111\n",
      "  676  554  333  732  263  567  422  602  705  724  180  584  205  279\n",
      "  470  215   90  880  147  300  137  485  388  255  125  186  773  296\n",
      "  114  655   89  275  159  566  221  393  610   59  389  671  565  250\n",
      "   63  486  116  714  187  158  400  178  293  105    9  408  376  491\n",
      "  626  697  247  368  432  330   39  511   64    4  405  459  690  469\n",
      "  615  101  572    2  386  353  235  510  375  771   71  295  253  621\n",
      "  305  650  790  521  372  578  518  179  472  289  373  302  490  458\n",
      "  283   49  436  634  426  410  367  583  248  600  107  328  515  474\n",
      "  260  798  451  401  239  720   87  273  211  173  417  306  249  648\n",
      "  526  604  326  344  641  331  895  638  365  776  336  774  519  941\n",
      "  637  750  691  355  455  411  345  286  575  627  343  468  361  202\n",
      "  687  784  664  704  767  342  370  622   67  121  366  381  548  360\n",
      "  884  440  781  412  531  818  581  614  667  329  431  154  450  665\n",
      "  226  611  189  520  544  497 1116  444  433  606  717  438  123  502\n",
      "   95  460  264  454  341  312  267  481  492  479  209  315  357  425\n",
      "  549  821  728  553  814  463  396  585  625  598  416  420  310  503\n",
      "  700  308  488  551  804  163  807  743  199  359  842  757  379  816\n",
      "  866  958  332  183  168  530  484  465  564  535  597  347  262  495\n",
      "  947  542  903  528  978  719  711  323  243  644  228  428  809  825\n",
      "  713  740  538  557   98  753  172  590  786  673  794  683  499  652\n",
      "  654  540]\n",
      "engine: Test has label encodes = [1116] which are not in train.\n",
      " => 1 rows\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17  0 19 20 21 22 23 24\n",
      " 25 26 27 28 30 33 34 35 36 38 39 52] [ 1 17  2  4 15  8  5  0  3  9 14  7 11 25 10 12 21 20 36 13 35 16  6 24\n",
      " 27 23 34 39 28 30 26 52 19 38 22 33]\n",
      "transmission: Test has label encodes = [] which are not in train.\n",
      "[2 3 1] [2 3 1]\n",
      "accident: Test has label encodes = [] which are not in train.\n",
      "[2 1] [2 1]\n",
      "clean_title: Test has label encodes = [] which are not in train.\n",
      "(135379, 10)\n",
      "(33845, 10)\n"
     ]
    }
   ],
   "source": [
    "# sns.boxplot(train['price'])\n",
    "# plt.show()\n",
    "\n",
    "# train_test_split, test is 0.2\n",
    "# test_raw = test_\n",
    "# train_raw = train_raw[:int(len(train_raw)*0.8)]\n",
    "train_data, test_data = preprocess(train_raw, test_raw)\n",
    "# summarize dataframe\n",
    "\n",
    "train_data.to_csv('C:/Projects/Kaggle/Used Car Prices/train_data.csv', index=False)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240910_085742\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.9.7\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          8\n",
      "Memory Avail:       5.94 GB / 15.85 GB (37.5%)\n",
      "Disk Space Avail:   352.90 GB / 475.76 GB (74.2%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240910_085742\"\n",
      "Train Data Rows:    55126\n",
      "Train Data Columns: 9\n",
      "Label Column:       price\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    6079.29 MB\n",
      "\tTrain Data (Original)  Memory Usage: 3.79 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 1 | ['milage']\n",
      "\t\t('int', [])   : 8 | ['id', 'model', 'model_year', 'fuel_type', 'engine', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 1 | ['milage']\n",
      "\t\t('int', [])       : 7 | ['id', 'model', 'model_year', 'fuel_type', 'engine', ...]\n",
      "\t\t('int', ['bool']) : 1 | ['clean_title']\n",
      "\t0.3s = Fit runtime\n",
      "\t9 features in original data used to generate 9 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.42 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.28s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.04535065123535174, Train Rows: 52626, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-33504.871\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.18s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-33900.3492\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.18s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 21658.4\n",
      "[2000]\tvalid_set's rmse: 21417.6\n",
      "[3000]\tvalid_set's rmse: 21333.1\n",
      "[4000]\tvalid_set's rmse: 21332.4\n",
      "[5000]\tvalid_set's rmse: 21329.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-21295.2265\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.5s\t = Training   runtime\n",
      "\t0.97s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 20727.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-20681.6098\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.57s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-22015.0407\t = Validation score   (-root_mean_squared_error)\n",
      "\t45.21s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-21004.469\t = Validation score   (-root_mean_squared_error)\n",
      "\t46.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-22004.6785\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.03s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-23128.7386\t = Validation score   (-root_mean_squared_error)\n",
      "\t151.16s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-20774.3232\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.69s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-23885.3316\t = Validation score   (-root_mean_squared_error)\n",
      "\t294.37s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t-20915.1104\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.07s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBM': 0.538, 'XGBoost': 0.385, 'LightGBMLarge': 0.077}\n",
      "\t-20593.5252\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 593.1s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 14631.7 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240910_085742\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "train_data = TabularDataset(train_data.drop(columns='id'))\n",
    "test_data = TabularDataset(test_data.drop(columns='id'))\n",
    "\n",
    "predictor = TabularPredictor(label='price', problem_type='regression').fit(train_data=train_data)\n",
    "predictions = predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150816    150816\n",
      "150817    150817\n",
      "150818    150818\n",
      "150819    150819\n",
      "150821    150821\n",
      "           ...  \n",
      "188527    188527\n",
      "188528    188528\n",
      "188529    188529\n",
      "188530    188530\n",
      "188532    188532\n",
      "Name: id, Length: 33845, dtype: int64\n",
      "150816    26914.552734\n",
      "150817    30722.593750\n",
      "150818    42519.640625\n",
      "150819    15488.376953\n",
      "150821    51928.785156\n",
      "              ...     \n",
      "188527    11303.687500\n",
      "188528    43035.425781\n",
      "188529    43773.972656\n",
      "188530    73905.648438\n",
      "188532    25546.533203\n",
      "Name: price, Length: 33845, dtype: float32\n",
      "RMSE: 19843.62616858485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diamo\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor.load(\"AutogluonModels/ag-20240910_085742\")\n",
    "# predictor.evaluate(test_data)\n",
    "# predictor.leaderboard(test_data)\n",
    "\n",
    "predicted = predictor.predict(test_data)\n",
    "# RMSE\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(test_data['id'])\n",
    "print(predicted)\n",
    "\n",
    "result = pd.concat((test_data['id'], predicted), axis=1)\n",
    "\n",
    "result.to_csv('result.csv')\n",
    "print('RMSE:', mean_squared_error(test_data['price'], predicted, squared=False)) # mean_squared_error(test_data['price'], predicted, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.6586062836180284\n",
      "RMSE: 76249.12764291337\n",
      "R^2: 0.5908319593168465\n",
      "RMSE: 76814.7313625287\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# add sklearn Ridge\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',  # Multiclass classification task\n",
    "    'metric': 'rmse',  # Logarithmic Loss as the evaluation metric for multiclass classification\n",
    "    'boosting_type': 'gbdt',\n",
    "    # 'early_stopping_rounds': 10,\n",
    "    'max_depth': 10, \n",
    "    'lambda_l1': 0.2, \n",
    "    'lambda_l2': 0.2, \n",
    "    'min_data_in_leaf': 20, \n",
    "    'min_gain_to_split': 0.01, \n",
    "    'feature_fraction': 0.5, \n",
    "    'bagging_fraction': 0.5,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "model = LGBMRegressor(**params)\n",
    "\n",
    "model.fit(train_data.drop('price', axis=1), train_data['price'])\n",
    "\n",
    "print('R^2:', model.score(train_data.drop('price', axis=1), train_data['price']))\n",
    "\n",
    "print('RMSE:', np.sqrt(mean_squared_error(test_data['price'], model.predict(test_data.drop('price', axis=1)))))\n",
    "\n",
    "model2 = Ridge()\n",
    "\n",
    "model2.fit(train_data.drop('price', axis=1), train_data['price'])\n",
    "\n",
    "print('R^2:', model2.score(train_data.drop('price', axis=1), train_data['price']))\n",
    "\n",
    "print('RMSE:', np.sqrt(mean_squared_error(test_data['price'], (model2.predict(test_data.drop('price', axis=1))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 77126.57351416007\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('RMSE:', np.sqrt(mean_squared_error(test_data['price'], model.predict(test_data.drop('price', axis=1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame:\n",
      "  model_engine  other_feature  engine_A  engine_B  engine_C  \\\n",
      "0            A              1         1         0         0   \n",
      "1            B              2         0         1         0   \n",
      "2            C              3         0         0         1   \n",
      "3            A              4         1         0         0   \n",
      "4            B              5         0         1         0   \n",
      "\n",
      "   model_engine_blank  \n",
      "0                   0  \n",
      "1                   0  \n",
      "2                   0  \n",
      "3                   0  \n",
      "4                   0  \n",
      "\n",
      "Test DataFrame:\n",
      "  model_engine  other_feature  engine_A  engine_B  engine_C  \\\n",
      "0            A              6         1         0         0   \n",
      "1            C              7         0         0         1   \n",
      "2            D              8         0         0         0   \n",
      "3            E              9         0         0         0   \n",
      "\n",
      "   model_engine_blank  \n",
      "0                   0  \n",
      "1                   0  \n",
      "2                   1  \n",
      "3                   1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrames\n",
    "train_df = pd.DataFrame({\n",
    "    'model_engine': ['A', 'B', 'C', 'A', 'B'],\n",
    "    'other_feature': [1, 2, 3, 4, 5]\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'model_engine': ['A', 'C', 'D', 'E'],\n",
    "    'other_feature': [6, 7, 8, 9]\n",
    "})\n",
    "\n",
    "# Get unique model_engine values from train set\n",
    "train_engines = set(train_df['model_engine'])\n",
    "\n",
    "# One-hot encode the 'model_engine' feature in the training set\n",
    "train_encoded = pd.get_dummies(train_df['model_engine'], prefix='engine')\n",
    "\n",
    "# Add the encoded columns to the train_df\n",
    "train_df = pd.concat([train_df, train_encoded], axis=1)\n",
    "\n",
    "# One-hot encode the 'model_engine' feature in the test set\n",
    "test_encoded = pd.get_dummies(test_df['model_engine'], prefix='engine')\n",
    "\n",
    "# Add the encoded columns to the test_df\n",
    "test_df = pd.concat([test_df, test_encoded], axis=1)\n",
    "\n",
    "# Add 'model_engine_blank' column to test_df\n",
    "test_df['model_engine_blank'] = test_df['model_engine'].apply(lambda x: 1 if x not in train_engines else 0)\n",
    "\n",
    "# Ensure test_df has the same columns as train_df (excluding 'model_engine_blank')\n",
    "for column in train_encoded.columns:\n",
    "    if column not in test_df.columns:\n",
    "        test_df[column] = 0\n",
    "\n",
    "# Add 'model_engine_blank' column to train_df (all zeros)\n",
    "train_df['model_engine_blank'] = 0\n",
    "\n",
    "# Ensure the order of columns in test_df matches train_df\n",
    "test_df = test_df[train_df.columns]\n",
    "\n",
    "# Fill missing columns in test_df with zeros\n",
    "test_df.fillna(0, inplace=True)\n",
    "\n",
    "# Display the final DataFrames\n",
    "print(\"Train DataFrame:\")\n",
    "print(train_df)\n",
    "print(\"\\nTest DataFrame:\")\n",
    "print(test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
