{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cleanse data and construct the model base on the results of EDA.\n",
    "\n",
    "First, I will create a baseline model. It puts data right into the model, which is LinearRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import copy\n",
    "\n",
    "train_raw = pd.read_csv('train.csv')\n",
    "test_raw = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_data(train_set, test_set, test_has_label=True):\n",
    "\n",
    "    \n",
    "\n",
    "    # 1. delete 'brand'\n",
    "    train_set = train_set.drop(['brand'], axis=1)\n",
    "    test_set = test_set.drop(['brand'], axis=1)\n",
    "\n",
    "    # 3. drop ext_col, int_col\n",
    "    train_set = train_set.drop(['ext_col', 'int_col'], axis=1)\n",
    "    test_set = test_set.drop(['ext_col', 'int_col'], axis=1)\n",
    "\n",
    "    # 4. drop id\n",
    "    train_set = train_set.drop(['id'], axis=1)\n",
    "    test_set = test_set.drop(['id'], axis=1)\n",
    "\n",
    "    ''' only 8 features ('model_year', 'milage', 'fuel_type', 'accident', 'clean_title', 'model', 'engine') left '''\n",
    "\n",
    "    # 5. for the same model value group, delete all rows that has high price (above 90% quantile).\n",
    "    quantiles = train_set.groupby('model')['price'].quantile(0.9)\n",
    "    train_set = train_set[train_set.apply(lambda row: row['price'] <= quantiles[row['model']], axis=1)]\n",
    "    \n",
    "    # 6. Filter out rows that has price higher than 500K\n",
    "    train_set = train_set[train_set.apply(lambda row: row['price'] < 500000, axis=1)]\n",
    "\n",
    "    # 7. convert 'year' to int, and calculate year passed from min(year)\n",
    "    base = min(train_set['model_year'])\n",
    "    train_set['model_year'] = (train_set['model_year'].astype(int) - base)\n",
    "    test_set['model_year']  = (test_set['model_year'] .astype(int) - base)\n",
    "    \n",
    "    # 9. Aggregate categorical features in transmission\n",
    "    col_names = [\n",
    "    'A/T',\n",
    "    'Transmission w/Dual Shift Mode',\n",
    "    '7-Speed A/T',\n",
    "    '8-Speed A/T',\n",
    "    '10-Speed Automatic',\n",
    "    '1-Speed A/T',\n",
    "    '6-Speed A/T',\n",
    "    '10-Speed A/T',\n",
    "    '9-Speed A/T',\n",
    "    '8-Speed Automatic',\n",
    "    '9-Speed Automatic',\n",
    "    '5-Speed A/T',\n",
    "    'Automatic',\n",
    "    '7-Speed Automatic with Auto-Shift',\n",
    "    'CVT Transmission',\n",
    "    '5-Speed M/T',\n",
    "    'M/T',\n",
    "    '6-Speed M/T',\n",
    "    '6-Speed Automatic',\n",
    "    '4-Speed Automatic',\n",
    "    '7-Speed M/T',\n",
    "    '2-Speed A/T',\n",
    "    '1-Speed Automatic',\n",
    "    'Automatic CVT',\n",
    "    '4-Speed A/T',\n",
    "    '6-Speed Manual',\n",
    "    'Transmission Overdrive Switch',\n",
    "    '8-Speed Automatic with Auto-Shift',\n",
    "    '7-Speed Manual',\n",
    "    '7-Speed Automatic',\n",
    "    '9-Speed Automatic with Auto-Shift',\n",
    "    '6-Speed Automatic with Auto-Shift',\n",
    "    '6-Speed Electronically Controlled Automatic with O',\n",
    "    'F',\n",
    "    'CVT-F',\n",
    "    '8-Speed Manual',\n",
    "    'Manual',\n",
    "    '-',\n",
    "    '2',\n",
    "    '6 Speed At/Mt',\n",
    "    '5-Speed Automatic',\n",
    "    '2-Speed Automatic',\n",
    "    '8-SPEED A/T',\n",
    "    '7-Speed',\n",
    "    'Variable',\n",
    "    'Single-Speed Fixed Gear',\n",
    "    '8-SPEED AT',\n",
    "    '10-Speed Automatic with Overdrive',\n",
    "    '7-Speed DCT Automatic',\n",
    "    'SCHEDULED FOR OR IN PRODUCTION',\n",
    "    '6-Speed',\n",
    "    '6 Speed Mt'\n",
    "]\n",
    "    \n",
    "    col_names_override = [\n",
    "    'A/T',\n",
    "    'Transmission w/Dual Shift Mode',\n",
    "    '7-Speed A/T',\n",
    "    '8-Speed A/T',\n",
    "    '10-Speed A/T',\n",
    "    '1-Speed A/T',\n",
    "    '6-Speed A/T',\n",
    "    '10-Speed A/T',\n",
    "    '9-Speed A/T',\n",
    "    '8-Speed A/T',\n",
    "    '9-Speed A/T',\n",
    "    '5-Speed A/T',\n",
    "    'A/T',\n",
    "    '7-Speed A/T with Auto-Shift',\n",
    "    'CVT Transmission',\n",
    "    '5-Speed M/T',\n",
    "    'M/T',\n",
    "    '6-Speed M/T',\n",
    "    '6-Speed A/T',\n",
    "    '4-Speed A/T',\n",
    "    '7-Speed M/T',\n",
    "    '2-Speed A/T',\n",
    "    '1-Speed A/T',\n",
    "    'A/T CVT',\n",
    "    '4-Speed A/T',\n",
    "    '6-SpeedM/T',\n",
    "    'Transmission Overdrive Switch',\n",
    "    '8-Speed A/T with Auto-Shift',\n",
    "    '7-Speed M/T',\n",
    "    '7-Speed A/T',\n",
    "    '9-Speed A/T with Auto-Shift',\n",
    "    '6-Speed A/T with Auto-Shift',\n",
    "    '6-Speed Electronically Controlled A/T with O',\n",
    "    '-',\n",
    "    'CVT-F',\n",
    "    '8-Speed M/T',\n",
    "    'M/T',\n",
    "    '-',\n",
    "    '-',\n",
    "    '6 Speed At/Mt',\n",
    "    '5-Speed A/T',\n",
    "    '2-Speed A/T',\n",
    "    '8-Speed A/T',\n",
    "    '7-Speed',\n",
    "    'Variable',\n",
    "    'Single-Speed Fixed Gear',\n",
    "    '8-Speed AT',\n",
    "    '10-Speed A/T with Overdrive',\n",
    "    '7-Speed DCT A/T',\n",
    "    '-',\n",
    "    '6-Speed',\n",
    "    '6 Speed M/T'\n",
    "]\n",
    "    \n",
    "    trans_dict = dict(zip(col_names, col_names_override))\n",
    "    train_set['transmission'] = train_set['transmission'].replace(trans_dict)\n",
    "    test_set['transmission']  = test_set['transmission'].replace(trans_dict)\n",
    "\n",
    "    # Get all numeric values\n",
    "    # I will add categoricals later\n",
    "    \n",
    "\n",
    "    \n",
    "    categoricals = ['accident', 'clean_title']#['transmission', 'fuel_type', 'accident', 'clean_title', 'model', 'engine']\n",
    "\n",
    "\n",
    "    data_average = train_set.groupby('model')['price'].mean().reset_index()\n",
    "    # print(data_average.shape)\n",
    "    data_average.columns = ['model', 'average_price']\n",
    "    print(data_average.head())\n",
    "    print(train_set.head())\n",
    "    replace_dict = dict(zip(data_average['model'], data_average['average_price']))\n",
    "    train_set['model'] = train_set['model'].replace(replace_dict)\n",
    "    print(train_set.head())\n",
    "    \n",
    "    # for the test data too:\n",
    "    data_average = train_set.groupby('model')['price'].mean().reset_index()\n",
    "    # print(data_average.shape)\n",
    "    data_average.columns = ['model', 'average_price']\n",
    "    test_set['model'] = test_set['model'].replace(replace_dict)\n",
    "    test_set['model'] = test_set['model'].apply(lambda x: 20000 if isinstance(x, str) else x)\n",
    "\n",
    "    train_processed = train_set.loc[:, ('model', 'milage', 'model_year', 'price')]\n",
    "    if test_has_label:\n",
    "        test_processed = test_set.loc[:, ('model', 'milage', 'model_year', 'price')]\n",
    "    else:\n",
    "        test_processed = test_set.loc[:, ('model', 'milage', 'model_year')]\n",
    "\n",
    "    # print(train_set.columns)\n",
    "\n",
    "    for i in categoricals:\n",
    "\n",
    "        train_set_part = train_set[i]\n",
    "        test_set_part = test_set[i]\n",
    "\n",
    "        train_set_part = train_set_part.fillna('blank')\n",
    "        test_set_part  = test_set_part.fillna('blank')\n",
    "\n",
    "        # set values of train_set[i] to 'blank', 1% of them\n",
    "        train_set_part.loc[train_set.sample(frac=0.01).index] = 'blank'\n",
    "\n",
    "        # Get all unique values in train_set\n",
    "        train_value_set = set(train_set_part)\n",
    "\n",
    "        # get_dummies for train's model\n",
    "        train_set_encoded = pd.get_dummies(train_set_part, prefix=i)\n",
    "\n",
    "        # Remove categories that are not in train, switch them to 'blank'\n",
    "        test_set[i] = test_set[i].apply(lambda x: 'blank' if x not in train_value_set else x)\n",
    "        test_set_encoded = pd.get_dummies(test_set[i], prefix=i)\n",
    "\n",
    "        # Add columns for train set\n",
    "        for j in train_set_encoded.columns:\n",
    "            if j not in test_set_encoded.columns:\n",
    "                test_set_encoded[j] = 0\n",
    "        \n",
    "        # Add blank columns if it was not created\n",
    "        if i+'_blank' not in train_set_encoded.columns:\n",
    "            train_set_encoded[i+'_blank'] = 0\n",
    "        if i+'_blank' not in test_set_encoded.columns:\n",
    "            test_set_encoded[i+'_blank'] = 0\n",
    "        \n",
    "        train_processed = pd.concat((train_processed, train_set_encoded), axis=1)\n",
    "        test_processed  = pd.concat((test_processed,  test_set_encoded), axis=1)\n",
    "\n",
    "    # sort columns\n",
    "    train_processed = train_processed[train_processed.columns.sort_values()]\n",
    "    test_processed  = test_processed[test_processed.columns.sort_values()]\n",
    "\n",
    "    print(train_processed.head())\n",
    "    \n",
    "\n",
    "    return train_processed, test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Refactoring of the function above...\n",
    "'''\n",
    "import copy\n",
    "\n",
    "def cleanse_data(train_set, test_set, test_has_label=True):\n",
    "\n",
    "    train_set = copy.deepcopy(train_set)\n",
    "    test_set = copy.deepcopy(test_set)\n",
    "    \n",
    "    # 1. delete 'brand'. This data is incorrect so often.\n",
    "    train_set = train_set.drop(['brand'], axis=1)\n",
    "    test_set = test_set.drop(['brand'], axis=1)\n",
    "\n",
    "    # 3. drop ext_col, int_col\n",
    "    train_set = train_set.drop(['ext_col', 'int_col'], axis=1)\n",
    "    test_set = test_set.drop(['ext_col', 'int_col'], axis=1)\n",
    "\n",
    "    # # 4. drop id\n",
    "    # train_set = train_set.drop(['id'], axis=1)\n",
    "    # test_set = test_set.drop(['id'], axis=1)\n",
    "\n",
    "    ''' only 8 features ('model_year', 'milage', 'fuel_type', 'accident', 'clean_title', 'model', 'engine') left '''\n",
    "\n",
    "    # 5. for the same model value group, delete all rows that has high price (above 90% quantile).\n",
    "    quantiles = train_set.groupby('model')['price'].quantile(0.9)\n",
    "    train_set = train_set[train_set.apply(lambda row: row['price'] <= quantiles[row['model']], axis=1)]\n",
    "    \n",
    "    # 6. Filter out rows that has price higher than 500K\n",
    "    train_set = train_set[train_set.apply(lambda row: row['price'] < 500000, axis=1)]\n",
    "\n",
    "    # 7. convert 'year' to int, and calculate year passed from min(year)\n",
    "    base = min(train_set['model_year'])\n",
    "    train_set['model_year'] = (train_set['model_year'].astype(int) - base)**2\n",
    "    test_set['model_year']  = (test_set['model_year'] .astype(int) - base)**2\n",
    "\n",
    "    # convert model name to the average of price\n",
    "    data_average = train_set.groupby('model')['price'].mean().reset_index()\n",
    "    data_average.columns = ['model', 'average_price']\n",
    "    replace_dict = dict(zip(data_average['model'], data_average['average_price']))\n",
    "\n",
    "    train_set['model'] = train_set['model'].replace(replace_dict)\n",
    "    test_set['model'] = test_set['model'].replace(replace_dict)\n",
    "\n",
    "    # If there is a model that was not in train set, set it to 20000\n",
    "    test_set['model'] = test_set['model'].apply(lambda x: 20000 if isinstance(x, str) else x)\n",
    "\n",
    "    # Change accident value\n",
    "    train_set.loc[train_set.sample(frac=0.01).index, 'accident'] = 'blank'\n",
    "    replace_dict = dict(zip(['At least 1 accident or damage reported', 'blank', 'None reported'], [1, 1, 0]))\n",
    "    train_set['accident'] = train_set['accident'].replace(replace_dict)\n",
    "    train_set['accident'].fillna(1, inplace=True)\n",
    "    test_set['accident'] = test_set['accident'].replace(replace_dict)\n",
    "    train_set['accident'].fillna(1, inplace=True)\n",
    "\n",
    "    # Change clean_title value\n",
    "    train_set.loc[train_set.sample(frac=0.01).index, 'clean_title'] = 'blank'\n",
    "    replace_dict = dict(zip(['Yes', 'blank', None], [1, 0, 0]))\n",
    "    train_set['clean_title'] = train_set['clean_title'].replace(replace_dict)\n",
    "    train_set['clean_title'].fillna(0, inplace=True)\n",
    "    test_set['clean_title'] = test_set['clean_title'].replace(replace_dict)\n",
    "    test_set['clean_title'].fillna(0, inplace=True)\n",
    "\n",
    "    train_set.drop(['engine', 'transmission', 'fuel_type'], axis=1, inplace=True)\n",
    "    test_set.drop(['engine', 'transmission', 'fuel_type'], axis=1, inplace=True)\n",
    "\n",
    "    for i in ['model', 'milage', 'model_year']:\n",
    "        scaler = StandardScaler()\n",
    "        train_set[i] = scaler.fit_transform(train_set[i].values.reshape(-1, 1))\n",
    "        test_set[i]  = scaler.transform(test_set[i].values.reshape(-1, 1))\n",
    "\n",
    "    return train_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train, test):\n",
    "\n",
    "    train = copy.deepcopy(train)\n",
    "    test = copy.deepcopy(test)\n",
    "\n",
    "    print(train.columns)\n",
    "\n",
    "    train = train.drop(columns='ext_col', axis=1)\n",
    "    train = train.drop(columns='int_col', axis=1)\n",
    "    train = train.drop(columns='brand', axis=1)\n",
    "    train = train.drop(columns='engine', axis=1)\n",
    "    train = train.drop(columns='transmission', axis=1)\n",
    "\n",
    "    test = test.drop(columns='ext_col', axis=1)\n",
    "    test = test.drop(columns='int_col', axis=1)\n",
    "    test = test.drop(columns='brand', axis=1)\n",
    "    test = test.drop(columns='engine', axis=1)\n",
    "    test = test.drop(columns='transmission', axis=1)\n",
    "\n",
    "    # Filter out rows that has price higher than 90% quantile.\n",
    "    quantiles = train.groupby('model')['price'].quantile(0.9)\n",
    "    train = train[train.apply(lambda row: row['price'] <= quantiles[row['model']], axis=1)].reset_index(drop=True)\n",
    "    \n",
    "    # 6. Filter out rows that has price higher than 500K\n",
    "    train = train[train.apply(lambda row: row['price'] < 500000, axis=1)].reset_index(drop=True)\n",
    "    \n",
    "    TRAIN_LN = int(len(train)*0.8)\n",
    "    CATS = [c for c in train.columns if not c in [\"id\",\"price\"] ]\n",
    "    NUMS = ['milage']\n",
    "    CATS = [c for c in CATS if not c in NUMS]\n",
    "    print(\"Categorical features:\", CATS )\n",
    "    print(\"Numerical features:\", NUMS)\n",
    "    print(\"STANDARDIZING: \",end=\"\")\n",
    "    for c in NUMS:\n",
    "        print(c,', ',end='')\n",
    "        m = train[c].mean()\n",
    "        s = train[c].std()\n",
    "        train[c] = (train[c]-m)/s\n",
    "        train[c] = train[c].fillna(m)\n",
    "    CAT_SIZE = []\n",
    "    CAT_EMB = []\n",
    "    RARE = []\n",
    "\n",
    "    print(\"LABEL ENCODING:\")\n",
    "    for c in CATS:\n",
    "        # LABEL ENCODE\n",
    "        train[c],_ = train[c].factorize()\n",
    "        # train[c] -= train[c].min()\n",
    "        vc = train[c].value_counts()\n",
    "\n",
    "        test[c],_ = test[c].factorize()\n",
    "        # test[c] -= test[c].min()\n",
    "        \n",
    "        # IDENTIFY RARE VALUES\n",
    "        RARE.append( vc.loc[vc<40].index.values )\n",
    "        n = train[c].nunique()\n",
    "        mn = train[c].min()\n",
    "        mx = train[c].max()\n",
    "        r = len(RARE[-1])\n",
    "        print(f'{c}: nunique={n}, min={mn}, max={mx}, rare_ct={r}')\n",
    "        \n",
    "        # RELABEL RARE VALUES AS ZERO\n",
    "        CAT_SIZE.append(mx+1 +1) #ADD ONE FOR RARE\n",
    "        CAT_EMB.append( int(np.ceil( np.sqrt(mx+1 +1))) ) # ADD ONE FOR RARE\n",
    "        train[c] += 1\n",
    "        test[c] += 1\n",
    "        print(train[c].isin(RARE[-1]))\n",
    "        train.loc[train[c].isin(RARE[-1]),c] = 0\n",
    "        test.loc[train[c].isin(RARE[-1]),c] = 0\n",
    "\n",
    "    # test = train.iloc[TRAIN_LN:]\n",
    "    # train = train.iloc[:TRAIN_LN]\n",
    "    \n",
    "    for c in CATS:\n",
    "        # COMPARE TEST CAT VALUES TO TRAIN CAT VALUES\n",
    "        A = train[c].unique()\n",
    "        B = test[c].unique()\n",
    "        print(A, B)\n",
    "        C = np.setdiff1d(B,A)\n",
    "        print(f\"{c}: Test has label encodes = {C} which are not in train.\")\n",
    "        if len(C)>0:\n",
    "            print(f\" => {len(test.loc[test[c].isin(C)])} rows\" )\n",
    "            \n",
    "        # RELABEL UNSEEN TEST VALUES AS ZERO\n",
    "        test.loc[test[c].isin(C),c] = 0 \n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diamo\\AppData\\Local\\Temp\\ipykernel_10728\\3754565328.py:42: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_set['model'] = train_set['model'].replace(replace_dict)\n",
      "C:\\Users\\diamo\\AppData\\Local\\Temp\\ipykernel_10728\\3754565328.py:51: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_set['accident'] = train_set['accident'].replace(replace_dict)\n",
      "C:\\Users\\diamo\\AppData\\Local\\Temp\\ipykernel_10728\\3754565328.py:52: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_set['accident'].fillna(1, inplace=True)\n",
      "C:\\Users\\diamo\\AppData\\Local\\Temp\\ipykernel_10728\\3754565328.py:53: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_set['accident'] = test_set['accident'].replace(replace_dict)\n",
      "C:\\Users\\diamo\\AppData\\Local\\Temp\\ipykernel_10728\\3754565328.py:54: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_set['accident'].fillna(1, inplace=True)\n",
      "C:\\Users\\diamo\\AppData\\Local\\Temp\\ipykernel_10728\\3754565328.py:59: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_set['clean_title'] = train_set['clean_title'].replace(replace_dict)\n",
      "C:\\Users\\diamo\\AppData\\Local\\Temp\\ipykernel_10728\\3754565328.py:60: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_set['clean_title'].fillna(0, inplace=True)\n",
      "C:\\Users\\diamo\\AppData\\Local\\Temp\\ipykernel_10728\\3754565328.py:61: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_set['clean_title'] = test_set['clean_title'].replace(replace_dict)\n",
      "C:\\Users\\diamo\\AppData\\Local\\Temp\\ipykernel_10728\\3754565328.py:62: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_set['clean_title'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169224, 7)\n",
      "(125690, 6)\n"
     ]
    }
   ],
   "source": [
    "# sns.boxplot(train['price'])\n",
    "# plt.show()\n",
    "\n",
    "# train_test_split, test is 0.2\n",
    "# test_raw = test_\n",
    "# train_raw = train_raw[:int(len(train_raw)*0.8)]\n",
    "train_data, test_data = cleanse_data(train_raw, test_raw, test_has_label=False)\n",
    "# summarize dataframe\n",
    "\n",
    "train_data.to_csv('C:/Projects/Kaggle/Used Car Prices/train_data.csv', index=False)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240911_022650\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.9.7\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          8\n",
      "Memory Avail:       5.98 GB / 15.85 GB (37.8%)\n",
      "Disk Space Avail:   355.89 GB / 475.76 GB (74.8%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (108303 samples, 6.06 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240911_022650\"\n",
      "Train Data Rows:    108303\n",
      "Train Data Columns: 5\n",
      "Label Column:       price\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    6125.04 MB\n",
      "\tTrain Data (Original)  Memory Usage: 4.13 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 5 | ['model', 'model_year', 'milage', 'accident', 'clean_title']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 3 | ['model', 'model_year', 'milage']\n",
      "\t\t('int', ['bool']) : 2 | ['accident', 'clean_title']\n",
      "\t0.1s = Fit runtime\n",
      "\t5 features in original data used to generate 5 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.69 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.023083386425122112, Train Rows: 105803, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 9 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-16842.8675\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-17886.7937\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t-15177.4998\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.57s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t-15163.441\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-16220.0279\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.77s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-15157.5422\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-16146.6313\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.53s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-14950.2461\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t-15126.3708\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'XGBoost': 0.923, 'RandomForestMSE': 0.077}\n",
      "\t-14940.9442\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 31.68s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 26496.6 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240911_022650\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "# test_id = test_data['id']\n",
    "\n",
    "if type(train_data) is not TabularDataset:\n",
    "    train_data = TabularDataset(train_data.drop(columns='id'))\n",
    "    test_data = TabularDataset(test_data.drop(columns='id'))\n",
    "    \n",
    "TRAIN_LN = int(0.8*len(train_data))\n",
    "valid_data = train_data[TRAIN_LN:]\n",
    "train_data = train_data[:TRAIN_LN]\n",
    "\n",
    "params = {\n",
    "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
    "\t'CAT': {},\n",
    "\t'XGB': {},\n",
    "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
    "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
    "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
    "}\n",
    "predictor = TabularPredictor(label='price', problem_type='regression').fit(train_data=train_data, hyperparameters=params)\n",
    "predictions = predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         188533\n",
      "1         188534\n",
      "2         188535\n",
      "3         188536\n",
      "4         188537\n",
      "           ...  \n",
      "125685    314218\n",
      "125686    314219\n",
      "125687    314220\n",
      "125688    314221\n",
      "125689    314222\n",
      "Name: id, Length: 125690, dtype: int64\n",
      "0         16587.445312\n",
      "1         67183.015625\n",
      "2         46520.953125\n",
      "3         25980.625000\n",
      "4         29084.169922\n",
      "              ...     \n",
      "125685    19927.986328\n",
      "125686    44360.398438\n",
      "125687    20882.740234\n",
      "125688    16210.500000\n",
      "125689    34473.070312\n",
      "Name: price, Length: 125690, dtype: float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diamo\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m result \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat((test_id, predictions), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m result\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# mean_squared_error(test_data['price'], prediction, squared=False)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diamo\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\diamo\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_regression.py:493\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    483\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    484\u001b[0m         (\n\u001b[0;32m    485\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msquared\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated in version 1.4 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[1;32m--> 493\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mroot_mean_squared_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m            \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m _check_reg_targets(\n\u001b[0;32m    498\u001b[0m     y_true, y_pred, multioutput\n\u001b[0;32m    499\u001b[0m )\n\u001b[0;32m    500\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\diamo\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diamo\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_regression.py:572\u001b[0m, in \u001b[0;36mroot_mean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    514\u001b[0m     {\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    524\u001b[0m ):\n\u001b[0;32m    525\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Root mean squared error regression loss.\u001b[39;00m\n\u001b[0;32m    526\u001b[0m \n\u001b[0;32m    527\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m    0.822...\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\n\u001b[1;32m--> 572\u001b[0m         \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m     )\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(multioutput, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m multioutput \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_values\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\diamo\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diamo\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_regression.py:497\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[0;32m    493\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[0;32m    494\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[0;32m    495\u001b[0m         )\n\u001b[1;32m--> 497\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    501\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\diamo\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_regression.py:103\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m--> 103\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\diamo\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1003\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    999\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1000\u001b[0m     )\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1003\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1012\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diamo\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\diamo\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m     )\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# predictor = TabularPredictor.load(\"AutogluonModels/ag-20240911_021401\")\n",
    "# predictor.evaluate(test_data)\n",
    "# predictor.leaderboard(test_data)\n",
    "\n",
    "# RMSE\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_id = test_data['id']\n",
    "\n",
    "print(test_id)\n",
    "print(predictions)\n",
    "\n",
    "result = pd.concat((test_id, predictions), axis=1)\n",
    "\n",
    "result.to_csv('result.csv', index=False)\n",
    "# print('RMSE:', mean_squared_error(test_data, predictions, squared=False)) # mean_squared_error(test_data['price'], prediction, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.6586062836180284\n",
      "RMSE: 76249.12764291337\n",
      "R^2: 0.5908319593168465\n",
      "RMSE: 76814.7313625287\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# add sklearn Ridge\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',  # Multiclass classification task\n",
    "    'metric': 'rmse',  # Logarithmic Loss as the evaluation metric for multiclass classification\n",
    "    'boosting_type': 'gbdt',\n",
    "    # 'early_stopping_rounds': 10,\n",
    "    'max_depth': 10, \n",
    "    'lambda_l1': 0.2, \n",
    "    'lambda_l2': 0.2, \n",
    "    'min_data_in_leaf': 20, \n",
    "    'min_gain_to_split': 0.01, \n",
    "    'feature_fraction': 0.5, \n",
    "    'bagging_fraction': 0.5,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "model = LGBMRegressor(**params)\n",
    "\n",
    "model.fit(train_data.drop('price', axis=1), train_data['price'])\n",
    "\n",
    "print('R^2:', model.score(train_data.drop('price', axis=1), train_data['price']))\n",
    "\n",
    "print('RMSE:', np.sqrt(mean_squared_error(test_data['price'], model.predict(test_data.drop('price', axis=1)))))\n",
    "\n",
    "model2 = Ridge()\n",
    "\n",
    "model2.fit(train_data.drop('price', axis=1), train_data['price'])\n",
    "\n",
    "print('R^2:', model2.score(train_data.drop('price', axis=1), train_data['price']))\n",
    "\n",
    "print('RMSE:', np.sqrt(mean_squared_error(test_data['price'], (model2.predict(test_data.drop('price', axis=1))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 77126.57351416007\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('RMSE:', np.sqrt(mean_squared_error(test_data['price'], model.predict(test_data.drop('price', axis=1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame:\n",
      "  model_engine  other_feature  engine_A  engine_B  engine_C  \\\n",
      "0            A              1         1         0         0   \n",
      "1            B              2         0         1         0   \n",
      "2            C              3         0         0         1   \n",
      "3            A              4         1         0         0   \n",
      "4            B              5         0         1         0   \n",
      "\n",
      "   model_engine_blank  \n",
      "0                   0  \n",
      "1                   0  \n",
      "2                   0  \n",
      "3                   0  \n",
      "4                   0  \n",
      "\n",
      "Test DataFrame:\n",
      "  model_engine  other_feature  engine_A  engine_B  engine_C  \\\n",
      "0            A              6         1         0         0   \n",
      "1            C              7         0         0         1   \n",
      "2            D              8         0         0         0   \n",
      "3            E              9         0         0         0   \n",
      "\n",
      "   model_engine_blank  \n",
      "0                   0  \n",
      "1                   0  \n",
      "2                   1  \n",
      "3                   1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrames\n",
    "train_df = pd.DataFrame({\n",
    "    'model_engine': ['A', 'B', 'C', 'A', 'B'],\n",
    "    'other_feature': [1, 2, 3, 4, 5]\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'model_engine': ['A', 'C', 'D', 'E'],\n",
    "    'other_feature': [6, 7, 8, 9]\n",
    "})\n",
    "\n",
    "# Get unique model_engine values from train set\n",
    "train_engines = set(train_df['model_engine'])\n",
    "\n",
    "# One-hot encode the 'model_engine' feature in the training set\n",
    "train_encoded = pd.get_dummies(train_df['model_engine'], prefix='engine')\n",
    "\n",
    "# Add the encoded columns to the train_df\n",
    "train_df = pd.concat([train_df, train_encoded], axis=1)\n",
    "\n",
    "# One-hot encode the 'model_engine' feature in the test set\n",
    "test_encoded = pd.get_dummies(test_df['model_engine'], prefix='engine')\n",
    "\n",
    "# Add the encoded columns to the test_df\n",
    "test_df = pd.concat([test_df, test_encoded], axis=1)\n",
    "\n",
    "# Add 'model_engine_blank' column to test_df\n",
    "test_df['model_engine_blank'] = test_df['model_engine'].apply(lambda x: 1 if x not in train_engines else 0)\n",
    "\n",
    "# Ensure test_df has the same columns as train_df (excluding 'model_engine_blank')\n",
    "for column in train_encoded.columns:\n",
    "    if column not in test_df.columns:\n",
    "        test_df[column] = 0\n",
    "\n",
    "# Add 'model_engine_blank' column to train_df (all zeros)\n",
    "train_df['model_engine_blank'] = 0\n",
    "\n",
    "# Ensure the order of columns in test_df matches train_df\n",
    "test_df = test_df[train_df.columns]\n",
    "\n",
    "# Fill missing columns in test_df with zeros\n",
    "test_df.fillna(0, inplace=True)\n",
    "\n",
    "# Display the final DataFrames\n",
    "print(\"Train DataFrame:\")\n",
    "print(train_df)\n",
    "print(\"\\nTest DataFrame:\")\n",
    "print(test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
